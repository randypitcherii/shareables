{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Unity Catalog Absolute Path Test\n\n**Goal**: Test whether a Delta table with absolute S3 paths pointing to multiple buckets works when registered in Unity Catalog.\n\n## Test Setup\n\nWe've created:\n- Parquet files in TWO different S3 buckets\n- A Delta log at `s3://your-bucket-east-1a/uc_test_delta/` that references:\n  - `s3://your-bucket-east-1a/uc_test/file1.parquet` (rows 1-3)\n  - `s3://your-bucket-east-1b/uc_test/file2.parquet` (rows 4-6)\n\n## Key Question\n\nWhen a Delta table's `add` actions contain **absolute paths** to files in buckets other than the table's LOCATION bucket, does Unity Catalog:\n\n1. **Read all 6 rows** (both buckets accessible via UC storage credentials)\n2. **Read only 3 rows** (only the LOCATION bucket is scoped)\n3. **Fail entirely** (UC rejects cross-bucket references)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Register in Unity Catalog\n",
    "\n",
    "Register the pre-created Delta table. The table's LOCATION is in `east-1a` bucket but references files in both `east-1a` and `east-1b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# Get configuration from environment or use placeholders\nBUCKET_EAST_1A = os.getenv(\"HIVE_EVAL_BUCKET_EAST_1A\", \"your-bucket-east-1a\")\nUC_CATALOG = os.getenv(\"HIVE_EVAL_UC_CATALOG\", \"your_catalog\")\nUC_SCHEMA = os.getenv(\"HIVE_EVAL_UC_SCHEMA\", \"your_uc_schema\")\nTABLE_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.uc_absolute_path_test\"\n\nprint(\"=== REGISTERING IN UNITY CATALOG ===\")\nprint(f\"Table: {TABLE_NAME}\")\n\n# Drop if exists\nspark.sql(f\"DROP TABLE IF EXISTS {TABLE_NAME}\")\n\n# Register the Delta table\nspark.sql(f\"\"\"\n    CREATE TABLE {TABLE_NAME}\n    USING DELTA\n    LOCATION 's3://{BUCKET_EAST_1A}/uc_test_delta/'\n\"\"\")\n\nprint(\"Table registered successfully\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify the table exists and show schema\nspark.sql(f\"DESCRIBE TABLE EXTENDED {TABLE_NAME}\").show(100, truncate=False)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Read via UC Table Name\n",
    "\n",
    "**This is the critical test.** When reading via the UC table name, UC provides credentials scoped to the table's LOCATION. The question is whether those credentials can access files stored in a DIFFERENT bucket that are referenced via absolute paths in the Delta log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== READING VIA UC TABLE NAME ===\")\nprint()\n\ntry:\n    result = spark.sql(f\"SELECT * FROM {TABLE_NAME}\")\n    count = result.count()\n    print(f\"SUCCESS! Row count: {count}\")\n    print()\n    result.show()\n    \n    print(\"\\nRows per bucket (CRITICAL - should be 3 and 3):\")\n    result.groupBy(\"bucket\").count().show()\n    \n    if count == 6:\n        print(\"\\n*** FINDING: UC CAN read cross-bucket absolute paths in Delta tables! ***\")\n    elif count == 3:\n        print(\"\\n*** FINDING: UC is scoped to LOCATION bucket - only same-bucket files work ***\")\n    else:\n        print(f\"\\n*** UNEXPECTED: Got {count} rows - investigate further ***\")\n        \nexcept Exception as e:\n    print(f\"FAILED!\")\n    print(f\"Error type: {type(e).__name__}\")\n    print(f\"Error message: {str(e)[:2000]}\")\n    print(\"\\n*** FINDING: UC rejects Delta tables with cross-bucket absolute paths ***\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test Write Operations (if read works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== TESTING INSERT ===\")\n\ntry:\n    spark.sql(f\"\"\"\n        INSERT INTO {TABLE_NAME}\n        VALUES (7, 'g', 'inserted')\n    \"\"\")\n    print(\"INSERT succeeded\")\n    \n    # Check new count\n    new_count = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n    print(f\"New row count: {new_count}\")\n    \n    # Show all data\n    spark.sql(f\"SELECT * FROM {TABLE_NAME} ORDER BY id\").show()\n    \nexcept Exception as e:\n    print(f\"INSERT FAILED: {str(e)[:500]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== TESTING UPDATE ===\")\n\ntry:\n    spark.sql(f\"\"\"\n        UPDATE {TABLE_NAME}\n        SET value = 'updated'\n        WHERE id = 1\n    \"\"\")\n    print(\"UPDATE succeeded\")\n    \n    # Show the updated row\n    spark.sql(f\"SELECT * FROM {TABLE_NAME} WHERE id = 1\").show()\n    \nexcept Exception as e:\n    print(f\"UPDATE FAILED: {str(e)[:500]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== TESTING DELETE ===\")\n\ntry:\n    spark.sql(f\"\"\"\n        DELETE FROM {TABLE_NAME}\n        WHERE id = 7\n    \"\"\")\n    print(\"DELETE succeeded\")\n    \n    # Show remaining rows\n    final_count = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n    print(f\"Final row count: {final_count}\")\n    \nexcept Exception as e:\n    print(f\"DELETE FAILED: {str(e)[:500]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== SUMMARY ===\")\nprint()\nprint(\"| Test | Works? | Row Count | Notes |\")\nprint(\"|------|--------|-----------|-------|\")\n\n# Test UC table SELECT\ntry:\n    uc_df = spark.sql(f\"SELECT * FROM {TABLE_NAME}\")\n    uc_count = uc_df.count()\n    if uc_count == 6:\n        print(f\"| UC table SELECT | YES | {uc_count} | Cross-bucket absolute paths WORK |\")\n    elif uc_count == 3:\n        print(f\"| UC table SELECT | PARTIAL | {uc_count} | Only LOCATION bucket accessible |\")\n    else:\n        print(f\"| UC table SELECT | YES | {uc_count} | Unexpected count - investigate |\")\nexcept Exception as e:\n    err = str(e)[:50].replace('|', '/')\n    print(f\"| UC table SELECT | NO | - | {err} |\")\n\nprint()\nprint(\"Interpretation:\")\nprint(\"- If count = 6: UC can read files from multiple buckets via absolute paths in Delta log\")\nprint(\"- If count = 3: UC scopes credentials to LOCATION bucket only\")\nprint(\"- If failed: UC rejects cross-bucket absolute paths entirely\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# Get bucket names from environment or use placeholders\nBUCKET_EAST_1A = os.getenv(\"HIVE_EVAL_BUCKET_EAST_1A\", \"your-bucket-east-1a\")\nBUCKET_EAST_1B = os.getenv(\"HIVE_EVAL_BUCKET_EAST_1B\", \"your-bucket-east-1b\")\nUC_CATALOG = os.getenv(\"HIVE_EVAL_UC_CATALOG\", \"your_catalog\")\nUC_SCHEMA = os.getenv(\"HIVE_EVAL_UC_SCHEMA\", \"your_uc_schema\")\nTABLE_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.uc_absolute_path_test\"\n\n# Drop the UC table\nspark.sql(f\"DROP TABLE IF EXISTS {TABLE_NAME}\")\nprint(\"UC table dropped\")\n\n# Note: S3 cleanup should be done separately if needed\nprint(\"\\nS3 data remains for re-running. To clean up:\")\nprint(f\"  aws s3 rm s3://{BUCKET_EAST_1A}/uc_test/ --recursive\")\nprint(f\"  aws s3 rm s3://{BUCKET_EAST_1B}/uc_test/ --recursive\")\nprint(f\"  aws s3 rm s3://{BUCKET_EAST_1A}/uc_test_delta/ --recursive\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}