{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34b243da",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ All SQL Example - Ready to copy/paste\n",
    "- You'll need to store your PAT in a secret\n",
    "- You'll of course want to update your namespace for the UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d3e2a2",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace function users.randy_pitcher.get_serving_endpoint_status(endpoint_name string, auth_token string)\n",
    "returns string\n",
    "language python\n",
    "as $$\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient(\n",
    "    host=\"https://e2-demo-field-eng.cloud.databricks.com\", \n",
    "    token=auth_token\n",
    ")\n",
    "\n",
    "try:\n",
    "    endpoint = w.serving_endpoints.get(name=endpoint_name)\n",
    "    return endpoint.state.ready.value if endpoint.state else \"UNKNOWN\"\n",
    "except Exception as e:\n",
    "    return f\"error: {str(e)}\"\n",
    "\n",
    "$$;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec477a1",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select \n",
    "  users.randy_pitcher.get_serving_endpoint_status(\n",
    "    'sd-bge-m3-encoder', \n",
    "    secret('fieldeng', 'rpw_pat')\n",
    "  )\n",
    "  \n",
    "union all\n",
    "\n",
    "select \n",
    "  users.randy_pitcher.get_serving_endpoint_status(\n",
    "    'advanced_mlops_churn_ep_mehul', \n",
    "    secret('fieldeng', 'rpw_pat')\n",
    "  );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dd122e",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Interactive python to experiment with the approach\n",
    "- try a few different things here to get an intuition for how it works\n",
    "\n",
    "### First, let's validate our connection to databricks\n",
    "- you can just skip this if you run this notebook directly in databricks\n",
    "- but if you work from cursor or vscode, this is a handy way to connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f822930f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Databricks SDK Workspace Client configured\n",
      "   Workspace URL: https://dbc-1f943136-5e42.cloud.databricks.com\n",
      "   Authentication Type: databricks-cli\n",
      "\n",
      "âœ… Databricks Connect Spark Session configured\n",
      "   Spark Version: 3.5.2\n",
      "   Spark Session active and ready to use\n",
      "\n",
      "ðŸ“Š Example usage:\n",
      "Workspace Client - List first 3 clusters:\n",
      "   - dbr_dev (State.RUNNING)\n",
      "   - Stupid Single User Cluster (State.RUNNING)\n",
      "   - dlt-execution-5a0ae2c7-2e2e-440f-a746-2bf5a32ee032 (State.TERMINATED)\n",
      "\n",
      "Spark Session - Sample query:\n",
      "+--------------------+\n",
      "|             message|\n",
      "+--------------------+\n",
      "|Hello from Databr...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configure Databricks SDK and Databricks Connect Clients\n",
    "\n",
    "# Configure Databricks SDK Workspace Client\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Initialize workspace client (will use default authentication flow)\n",
    "# This will automatically try: notebook-native auth, then environment variables, then .databrickscfg\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Configure Databricks Connect Spark Session\n",
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "# Initialize Spark session (will use default authentication flow)\n",
    "# This will automatically use the same authentication as the workspace client\n",
    "spark = DatabricksSession.builder.getOrCreate()\n",
    "\n",
    "# Verify connections\n",
    "print(\"âœ… Databricks SDK Workspace Client configured\")\n",
    "print(f\"   Workspace URL: {w.config.host}\")\n",
    "print(f\"   Authentication Type: {w.config.auth_type}\")\n",
    "\n",
    "print(\"\\nâœ… Databricks Connect Spark Session configured\")\n",
    "print(f\"   Spark Version: {spark.version}\")\n",
    "print(\"   Spark Session active and ready to use\")\n",
    "\n",
    "# Example usage of both clients\n",
    "print(\"\\nðŸ“Š Example usage:\")\n",
    "print(\"Workspace Client - List first 3 clusters:\")\n",
    "clusters = w.clusters.list()\n",
    "for i, cluster in enumerate(clusters):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print(f\"   - {cluster.cluster_name} ({cluster.state})\")\n",
    "\n",
    "print(\"\\nSpark Session - Sample query:\")\n",
    "df = spark.sql(\"SELECT 'Hello from Databricks Connect!' as message\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46181c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'customer_outreach_ai_builder', 'state': 'READY'},\n",
       " {'name': 'kie-d5861e35-endpoint', 'state': 'READY'},\n",
       " {'name': 'OpenAI_gpt4', 'state': 'READY'},\n",
       " {'name': 'databricks-claude-3-7-sonnet', 'state': 'READY'},\n",
       " {'name': 'databricks-claude-sonnet-4', 'state': 'READY'},\n",
       " {'name': 'databricks-llama-4-maverick', 'state': 'READY'},\n",
       " {'name': 'databricks-gemma-3-12b', 'state': 'READY'},\n",
       " {'name': 'databricks-claude-opus-4', 'state': 'READY'},\n",
       " {'name': 'databricks-meta-llama-3-1-8b-instruct', 'state': 'READY'},\n",
       " {'name': 'databricks-meta-llama-3-3-70b-instruct', 'state': 'READY'},\n",
       " {'name': 'databricks-gte-large-en', 'state': 'READY'},\n",
       " {'name': 'databricks-meta-llama-3-1-405b-instruct', 'state': 'READY'},\n",
       " {'name': 'databricks-bge-large-en', 'state': 'READY'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get all model serving endpoints\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    "endpoints = w.serving_endpoints.list()\n",
    "display([{\"name\": ep.name, \"state\": ep.state.ready.value if ep.state else \"UNKNOWN\"} for ep in endpoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfdc156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the status of a model serving endpoint\n",
    "def get_serving_endpoint_status(endpoint_name: str) -> str:\n",
    "  try:\n",
    "    w = WorkspaceClient()\n",
    "    endpoint = w.serving_endpoints.get(name=endpoint_name)\n",
    "    return endpoint.state.ready.value if endpoint.state else \"UNKNOWN\"\n",
    "  except Exception as e:\n",
    "    return f\"ERROR: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1732b1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'READY'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# give the function a try\n",
    "display(get_serving_endpoint_status('databricks-gemma-3-12b'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
